# Experimenting with Deep Architectures

## Phase 1
Implement a Multi-Layered Neural Network with basic backpropagation trained with Stochastic Gradient Descent (SGD).

Things to add (in order of priority):
 - Dropout during Training
 - Regularization in Cost function
 - Momentum
 
## Phase 2
Augment Network by pre-training

To be tried:
 - Stacking Denoising Auto-Encoders
 - Stacking RBMs

